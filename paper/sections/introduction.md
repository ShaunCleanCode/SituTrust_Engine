# Introduction

## **Problem Definition**

Large Language Models (LLMs) have achieved remarkable fluency and task-general abilities, yet their usage in practice remains dominated by a *one user : one model* prompting paradigm . In typical deployments (e.g. ChatGPT), a human must iteratively provide prompts and guidance at each step, because the model on its own does not autonomously plan multi-step solutions or maintain long-term goals . This 1:1 interaction pattern means complex tasks still require constant human intervention to break down problems, verify outputs, and steer the conversation. In other words, current LLMs excel in single-turn or short dialogue settings, but struggle to **carry out extended, multi-step reasoning without continuous user input**. This limitation has become a bottleneck for truly autonomous AI agents, motivating research into new paradigms that reduce human oversight in LLM-driven workflows .

Recent efforts have begun to address this autonomy gap by chaining LLM reasoning steps or introducing multiple LLMs. For example, *AutoGPT* emerged as an early attempt to let an LLM agent “self-prompt” toward a high-level goal without user intervention . Unlike a standard chatbot that awaits each human prompt, AutoGPT can recursively generate its own next subtask instructions, giving a glimpse of what fully autonomous LLM usage might look like . The appeal of such autonomy is clear – a user could simply specify an objective (e.g. *“Plan my travel itinerary”*) and the agent would handle the rest. **However, these initial solutions also highlight the deeper challenges**: LLMs operating on their own tend to lose coherence on long tasks, lack grounding in persistent memory, and can get stuck without external feedback . In summary, the field is grappling with how to move beyond the 1:1 prompting model toward more *self-directed, collaborative, and long-lived* LLM behaviors.

## **Limitations of Prior Work**

Several research directions have explored pathways to more autonomous LLM systems, but each comes with limitations that **SituTrust** aims to overcome. Single-agent strategies like **ReAct** and **Toolformer** augment an LLM’s capabilities but do so with significant external support. In the ReAct paradigm, a single model is prompted to interweave *reasoning traces*with *actions* (e.g. API calls) in its output . This allows the model to plan and execute sub-tasks (querying tools or knowledge bases) as part of its chain-of-thought . While ReAct improves the agent’s ability to handle multi-step queries with less human micro-management, it still relies on an external orchestration loop to read the model’s “Act” outputs and feed results back in. The model itself does not truly self-organize; it operates within a step-by-step script governed by the prompt and the surrounding program. Similarly, Toolformer demonstrated that an LM can be *trained* to decide when to call external tools and incorporate the results into its generation . But Toolformer requires additional fine-tuning (albeit self-supervised) to integrate tool-use abilities , meaning it is not a prompt-only solution. More importantly, neither ReAct nor Toolformer consider scenarios with **multiple agents** – they focus on a single LLM “agent” augmented by tools or reasoning, without peer collaboration or social dynamics.

Multi-agent systems (MAS) based on LLMs take a different approach: they instantiate *multiple* specialized LLM agents that communicate to solve tasks cooperatively . Prior MAS frameworks like **AutoGPT**, **ChatDev**, **MetaGPT** and the CAMEL agent society have delivered promising case studies – from software design to role-playing dialogues – by assigning roles to different LLM agents and letting them interact in natural language . For instance, *ChatDev* simulates a software team (planner, coder, tester, etc.) where each agent is an LLM role that chats with others to develop a program . *MetaGPT* encodes human procedural knowledge (Standard Operating Procedures) into prompts to structure a multi-agent “software company,” thereby orchestrating agents through an assembly-line workflow . These systems **prove that groups of LLM agents can, in principle, achieve complex, collaborative behavior**, often outperforming single-agent approaches on creative problem-solving . In fact, recent studies report that a team of role-playing LLMs can yield *higher reasoning accuracy and adaptability* than a lone model, by leveraging diverse perspectives and cross-verification among agents .

Despite their advances, prior multi-agent frameworks suffer two key shortcomings. **First, they lack persistent trust modeling** – *LLM agents in these systems generally treat every message from others at face value, without any built-in notion of how trustworthy each peer is* . This can lead to cascades of errors or deception: if one agent produces a faulty statement, others may accept it uncritically, since no mechanism exists to evaluate credibility. Recent analyses have highlighted this vulnerability, noting that current LLM agents “treat all incoming messages equally without evaluating their trustworthiness” . Some works have started exploring heuristics for trust (e.g. filtering obviously harmful content), but a holistic, persistent model of inter-agent trust has been missing . **Second, prior systems rely on external orchestration and fixed protocols** to manage the agents’ cooperation. In frameworks like AutoGPT or MetaGPT, a hard-coded loop or controller delegates tasks, parses outputs, and feeds them among agents in a predetermined manner . Agents do not truly self-organize their interaction patterns; instead, the “collaboration” is largely imposed by a structured script (e.g. turn-taking chat rounds, tool-use checkpoints, or a manager/worker hierarchy). This external scaffolding not only complicates the system, but also limits the flexibility of agent interactions. Without an orchestrator, agents might misalign or talk past each other, so prior works enforced alignment via rule-based turn schedules, shared memory databases, or explicit leader roles . In summary, existing multi-agent LLM approaches have **not yet achieved a native, free-form collaboration paradigm** – they either ignore the crucial dimension of *trust* between agents, or they depend on non-trivial intervention (through code or fine-tuning) outside the prompt to keep agents coordinated.

## **Novelty: The SituTrust Approach**

We introduce **SituTrust**, a prompt-native multi-agent system designed to address the above gaps by enabling *situated, trust-aware collaboration* among LLM agents **entirely within a single prompt**. SituTrust’s novelty lies in three key ideas:

- **Spatial Prompting for Scene Construction:** In SituTrust, we harness the spatial and situational reasoning abilities of LLMs by explicitly constructing a *shared scene* within the prompt. All agents are described as co-present in a virtual environment, with a spatial layout or context relevant to the task. This **spatial prompting** acts as a common ground for the agents’ interactions – for example, a prompt might describe a virtual “workspace” or map where each agent is located at a certain position with certain observable information. By framing the collaboration in a spatial metaphor, we allow the LLM to leverage its world knowledge and intuitive physics/common-sense priors to ground the conversation . Recent work suggests that LLMs can adapt to environment factors and maintain coherent plans when prompts include structured situational information . Our approach builds on this insight: SituTrust agents perceive and discuss a shared scene (e.g. a whiteboard, objects, or locations), which helps regulate turn-taking and reference, much like humans collaborating in a physical space. This prompt-embedded “world” provides continuity across the dialogue and enables **spatially aware reasoning** (agents can point to things, divide areas of work, move within the scene, etc.), all without any external simulator – the *scene exists purely in the prompt text*. By using spatial context as part of the prompt, SituTrust moves away from abstract chat and towards a more **situated form of multi-agent reasoning**, which we hypothesize reduces misunderstandings and encourages emergent coordination.
- **Relational Vector Prompting for Trust Memory:** SituTrust introduces a mechanism for persistent trust modeling *within* the prompt called **Relational Vector Prompting**. The core idea is to maintain a *trust memory embedding* for each pair of agents as part of the conversation state. We encode the evolving trust or confidence an agent has in its peers as a vector of attributes (for example, reflecting past reliability, expertise, or honesty), and represent these attributes in the prompt in a human-readable form (such as a summary table or descriptor in each agent’s profile). As the dialogue progresses, agents update these trust descriptors based on interactions – effectively performing a form of **on-the-fly trust assessment** entirely through prompting. This approach is inspired by recent findings that certain internal attention heads of LLMs correspond to evaluating trustworthiness along specific dimensions . Instead of modifying the model’s architecture, we leverage the base model’s capacity to handle and update structured information in the prompt. Each agent’s prompt context includes a section like “Trust toward others: Agent B = High (expert in domain X), Agent C = Low (prior mistake in Y)” which gets revised as events unfold. These *relational vectors* (as textual representations) bias the agent’s subsequent responses – for instance, an agent will defer to or double-check an answer from a peer it trusts less. To our knowledge, SituTrust is the first system to **embed dynamic inter-agent trust directly into the prompt context**. This allows **persistent trust modeling**without any training or external verifier: the LLM’s own language processing is used to carry forward and apply trust indicators. By implementing trust as part of the prompt, agents gain a form of social memory, addressing the vulnerability where they would otherwise treat all peer messages as equally credible . We anticipate that this trust-aware prompting will improve collaboration efficiency (agents focus on the most credible information) and robustness in the face of errors or deceitful inputs.
- **Self-Organizing Agent-to-Agent Collaboration:** Perhaps most importantly, SituTrust demonstrates *fully self-organizing* multi-agent collaboration **with no external orchestration or fine-tuning**. All coordination is achieved through the prompt design and the emergent dynamics of the LLM agents’ conversation. Once the initial prompt establishes the scene and trust embeddings, the agents are prompted to **autonomously converse, plan, and divide tasks** to achieve the user’s goal. There is no outside “manager” process deciding which agent speaks when or how the task is split – these decisions arise from the agents’ own dialogue within the situational and trust context given. This is a departure from prior systems that hard-coded interaction protocols . Instead, SituTrust uses few-shot examples and role instructions to *nudge* agents toward productive behaviors (e.g. proposing a plan, consulting others, raising doubts if trust is low), but ultimately lets the LLM’s next-token prediction model govern the flow. In essence, the prompt itself serves as the **only governing mechanism**, and the collaboration is an emergent property of the prompt-conditioned generation. By not requiring any retraining or plugin, SituTrust can be deployed on off-the-shelf LLMs (like GPT-4 or LLaMA) and still achieve sophisticated teamwork purely through zero-shot prompt engineering. This novel design opens the door to studying *emergent social behavior in LLMs* under minimal assumptions. We relate this to the “agent societies” seen in works like Generative Agents , but in our case the entire society runs in one unified prompt context rather than a bespoke simulation with separate memory stores. SituTrust agents have been observed to spontaneously negotiate roles, cross-verify each other’s outputs, and adapt their strategy when faced with new information – all behaviors that arise without an explicit script. By enabling such **free-form agent-to-agent (A2A) collaboration**, SituTrust more closely resembles a group of humans brainstorming in a room, as opposed to a rigid turn-based protocol.

## **Contribution and Impact**

SituTrust represents a significant step toward *prompt-native multi-agent intelligence*. In this work, we introduce a system that **simulates intelligent teamwork within a single LLM prompt**, overcoming many limitations of previous 1:1 prompting and orchestrated agent frameworks. The main contributions of our approach can be summarized as follows:

- **Prompt-Native Multi-Agent System:** We propose *SituTrust*, a novel architecture that requires no model fine-tuning or external controllers to enable multi-agent collaboration. It leverages only prompt engineering and the base LLM to facilitate a team of agents that can reason, communicate, and coordinate actions autonomously. To our knowledge, this is one of the first demonstrations of an entirely prompt-driven MAS with dynamic inter-agent interactions.
- **Spatial and Trust-Aware Prompting:** We develop innovative prompting techniques – Spatial Prompting and Relational Vector Prompting – that incorporate a shared scene and trust memory into the context. By grounding the discourse in a spatial scenario, we improve coherence and allow emergent organization (e.g. agents dividing tasks by “location” or component). By maintaining trust vectors in the prompt, we imbue agents with social intelligence to weigh each other’s contributions, which is a new capability beyond the scope of standard LLM dialogues .
- **Autonomous Emergent Coordination:** We demonstrate that SituTrust agents can achieve creative, distributed problem-solving in a fully *zero-shot* manner. Without any training on coordination games, the agents in our system spontaneously assign roles, collaborate on subgoals, and even exhibit **emergent behaviors** akin to human teams (such as forming consensus or double-checking a low-trust peer’s claim). Prior work has hinted at such emergent coordination – for example, a recent sandbox simulation showed that simple prompting can lead multiple agents to organize a social event autonomously . SituTrust achieves similar emergence **within a single prompt**, which is considerably more lightweight and accessible. This suggests that complex coordination among AI agents need not rely on heavy infrastructure: it can arise from clever prompt design.

The impact of SituTrust is twofold. First, it provides a **practical template for zero-shot multi-agent collaboration**: developers can deploy a team of LLM agents to tackle a task by simply formatting a prompt (with a scene description and trust table), without writing elaborate orchestration code. This lowers the barrier to experimenting with multi-agent AI and could inspire new applications where AI agents brainstorm or work together creatively in real time. Second, SituTrust lays the **groundwork for future research on zero-shot coordination and social dynamics** in LLMs. By observing how untrained agents behave in our prompt-simulated scenarios, we gain insight into the latent capabilities and failure modes of LLM societies. Going forward, this approach can be used to study problems like cooperative planning, communication emergence, and trust calibration among AI agents *before* deploying them in the wild. We envision SituTrust as a stepping stone toward more **robust, trustworthy multi-agent AI**, ultimately contributing to the development of LLM systems that coordinate as fluidly and safely as human teams in open-ended tasks.

Overall, SituTrust pushes the envelope of what can be achieved through prompt engineering alone, showing that with the right implicit scaffolding (situational context and trust memory), LLMs can orchestrate themselves into a collaborative ensemble. We hope this work sparks further exploration into prompt-native agent societies and accelerates progress on autonomous coordination – *without* the need for constant human prompt guidance or costly model retraining.